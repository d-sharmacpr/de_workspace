{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1510e42-77c5-41a8-b610-576c1c0e1d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField,StructType, StringType, IntegerType, FloatType, LongType, DateType\n",
    "\n",
    "# from pyspark.context import SparkContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "# print(sc)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"scd2\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0e3c32-7568-431b-9e7f-f107b2951632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql.types import DataType,TimestampType\n",
    "data = [\n",
    "    (101, 'abhishek', 'Samsung', 40000, date(2021, 1, 1), date(2022, 1, 5)),\n",
    "    (102, 'dk', 'Indev Consultancy', 45000, date(2021, 1, 14), date(9999, 1, 1)),\n",
    "    (103, 'rajan', 'Capgemini', 28500, date(2021, 8, 1), date(9999, 1, 1)),\n",
    "    (104, 'vikash', 'Product', 55000, date(2021, 9, 1), date(9999, 1, 1))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('company', StringType(), True),\n",
    "    StructField('sal', IntegerType(), True),\n",
    "    StructField('start_date', DateType(), True),\n",
    "    StructField('end_date', DateType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab8951f-4f35-4eff-a42b-b445d1b5b323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, 'abhishek', 'Samsung', 40000, date(2021, 1, 1)),\n",
    "    (102, 'dk', 'Accenture', 56000, date(2021, 6, 14)),\n",
    "    (103, 'rajan', 'Capgemini', 55000, date(2023, 12, 7)),\n",
    "    (105, 'saurabh', 'Kpi tech', 35000, date(2024, 6, 11))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('company', StringType(), True),\n",
    "    StructField('sal', IntegerType(), True),\n",
    "    StructField('start_date', DateType(), True)\n",
    "])\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema)\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e99cbb0-223e-4b32-bc8d-add5051e294e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('map_has',xxhash64(col('id'), col('name'), col('company'), col('sal')))\n",
    "df1 = df1.withColumn('map_has',xxhash64(col('id'), col('name'), col('company'), col('sal')))\n",
    "display(df)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a51c23-c175-47fd-9a87-bc837a78c61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = df1.join(df,['map_has'],'leftanti')\n",
    "df_new = df_new.drop('map_has')\n",
    "df = df.drop('map_has')\n",
    "display(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb796472-78b5-44ef-b66a-d99922325a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.withColumn('end_date',lit(date(9999,1,1)))\n",
    "df = df.union(df_new)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac740ad-939f-4945-856c-6de6fe6d0208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "win = Window.partitionBy('id').orderBy(('start_date'))\n",
    "df = df.withColumn('rank', row_number().over(win))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d682d2fe-a2fd-4e53-80e6-e5ae59d197e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b75f844-c353-4b14-90e4-ab0d23e11c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, when, lag\n",
    "\n",
    "win = Window.partitionBy('id').orderBy('start_date')\n",
    "df = df.withColumn('end_date', when(col('rank')==2,lag(col('start_date'), 1).over(win)) \\\n",
    "    .otherwise(col('end_date')))\n",
    "\n",
    "df = df.drop('rank','lag')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1509d6e0-f7ef-4c94-b0fa-438d7476f01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scd2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
